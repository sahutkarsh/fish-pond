{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-agent setup where the closed environment contains a fish pond and agents. The agents are allowed to consume the fishes in the pond. The reproduction rate for fishes is directly proportional to the population left (R $\\propto$ N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Actions: <br>\n",
    "\n",
    "0. Move Left\n",
    "1. Move Right\n",
    "2. Move Up\n",
    "3. Move Down\n",
    "4. Eat Fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment State:\n",
    "\n",
    "0. Agent Position (X)\n",
    "1. Agent Position (Y)\n",
    "2. Fish Population\n",
    "3. Agent Health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward:\n",
    "\n",
    "The reward is +1 for surviving every time-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode Termination: \n",
    "\n",
    "1. All the Agents Die\n",
    "2. Fish Population becomes Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FishPondEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FishPondEnv, self).__init__()\n",
    "        self.size = 20\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        low = np.array([0, 0, 0, 0])\n",
    "        high = np.array([self.size, int(self.size * 0.75), np.iinfo(int).max, 100])\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def check_termination(health, fish_population):\n",
    "        condition1 = health <= 0\n",
    "        condition2 = population < 1\n",
    "        return (condition1 or condition2)\n",
    "        \n",
    "    def step(self, action):\n",
    "        dist = 1 # Step Size of the Agent\n",
    "        hunger = 1 # Depletion of Health per time-step\n",
    "        nutrition = 5 # Improvement in Health by consumption of one fish\n",
    "        x, y, fish_population, health = self.state\n",
    "        x = (x - dist) if (action == 1) else (x + dist) if (action == 2) else x\n",
    "        y = (y + dist) if (action == 3) else (y - dist) if (action == 4) else y\n",
    "        # Doubt: To check for limits of co-ordinates or not since it is specified in the observation space?\n",
    "        regeneration_rate = 0.1 * fish_population\n",
    "        fish_population += (regeneration_rate * 1)\n",
    "        health -= hunger\n",
    "        if (action == 5 and fish_population >= 1):\n",
    "            health += nutrition\n",
    "            fish_population -= 1\n",
    "        self.state = np.array([x, y, fish_population, health])\n",
    "        done = check_termination(health, fish_population)\n",
    "        \n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Terminated\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"step() called while environment has already returned, call reset() first\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        \n",
    "        return np.array([self.state, reward, done, info])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0, 10, 100]) # Initial Environment Conditions\n",
    "        self.steps_beyond_done = None\n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
